{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepSpeedMultiModalityPerceiver.ipynb","provenance":[],"authorship_tag":"ABX9TyPPsIb4Hz67xbGQscNS4/N8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2-xn6jx2BKY1","executionInfo":{"status":"ok","timestamp":1618753473415,"user_tz":240,"elapsed":3168,"user":{"displayName":"Fabien Campagne","photoUrl":"","userId":"07590698054228565474"}},"outputId":"8c3d3e0e-cea9-41bb-bfab-565c97bf08f7"},"source":["! pip install deepspeed perceiver-multi-modality-pytorch==1.1.0 "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: deepspeed in /usr/local/lib/python3.7/dist-packages (0.3.14)\n","Requirement already satisfied: perceiver-multi-modality-pytorch==1.1.0 in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.19.5)\n","Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.10.0.post2)\n","Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.7.1+cu110)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed) (4.41.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deepspeed) (5.4.8)\n","Requirement already satisfied: tensorboardX==1.8 in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.8)\n","Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deepspeed) (0.9.0+cu111)\n","Requirement already satisfied: einops<0.4,>=0.3 in /usr/local/lib/python3.7/dist-packages (from perceiver-multi-modality-pytorch==1.1.0) (0.3.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->deepspeed) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed) (1.15.0)\n","Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed) (3.12.4)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->deepspeed) (8.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed) (54.2.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rf0dW-IoFzr3","executionInfo":{"status":"ok","timestamp":1618753478955,"user_tz":240,"elapsed":4667,"user":{"displayName":"Fabien Campagne","photoUrl":"","userId":"07590698054228565474"}},"outputId":"be63c45c-fd09-40be-f5ee-4c5129c0fa67"},"source":["!pip install mpi4py\n","!!pip install torch==1.7.1+cu110  -f https://download.pytorch.org/whl/torch_stable.html\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: mpi4py in /usr/local/lib/python3.7/dist-packages (3.0.3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Looking in links: https://download.pytorch.org/whl/torch_stable.html',\n"," 'Requirement already satisfied: torch==1.7.1+cu110 in /usr/local/lib/python3.7/dist-packages (1.7.1+cu110)',\n"," 'Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.19.5)',\n"," 'Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)']"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4zL1FKtB8DC","executionInfo":{"status":"ok","timestamp":1618753481090,"user_tz":240,"elapsed":2921,"user":{"displayName":"Fabien Campagne","photoUrl":"","userId":"07590698054228565474"}},"outputId":"3fc15d06-a51c-4170-a857-a7137faf8b1a"},"source":["!ds_report"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--------------------------------------------------\n","DeepSpeed C++/CUDA extension op report\n","--------------------------------------------------\n","NOTE: Ops not installed will be just-in-time (JIT) compiled at\n","      runtime if needed. Op compatibility means that your system\n","      meet the required dependencies to JIT install the op.\n","--------------------------------------------------\n","JIT compiled ops requires ninja\n","ninja .................. \u001b[92m[OKAY]\u001b[0m\n","--------------------------------------------------\n","op name ................ installed .. compatible\n","--------------------------------------------------\n","cpu_adam ............... \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","fused_adam ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","fused_lamb ............. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires one of the following commands '['llvm-config', 'llvm-config-9']', but it does not exist!\n","\u001b[93m [WARNING] \u001b[0m sparse_attn requires CUDA version 10.1+, does not currently support >=11 or <10.1\n","sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n","transformer ............ \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","stochastic_transformer . \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","utils .................. \u001b[93m[NO]\u001b[0m ....... \u001b[92m[OKAY]\u001b[0m\n","--------------------------------------------------\n","DeepSpeed general environment info:\n","torch install path ............... ['/usr/local/lib/python3.7/dist-packages/torch']\n","torch version .................... 1.7.1+cu110\n","torch cuda version ............... 11.0\n","nvcc version ..................... 11.0\n","deepspeed install path ........... ['/usr/local/lib/python3.7/dist-packages/deepspeed']\n","deepspeed info ................... 0.3.14, unknown, unknown\n","deepspeed wheel compiled w. ...... torch 1.8, cuda 10.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Sfl1gMZDCCpO","executionInfo":{"status":"error","timestamp":1618755260313,"user_tz":240,"elapsed":959,"user":{"displayName":"Fabien Campagne","photoUrl":"","userId":"07590698054228565474"}},"outputId":"f1776e84-9503-400b-f8e6-7024c5dc5d9d"},"source":["\n","num_epochs=10\n","\n","from perceiver_pytorch.multi_modality_perceiver import  InputModality\n","from perceiver_pytorch.multi_modality_with_text_perceiver import MultiModalityWithTextPerceiver, InputModalityWithEmbedding\n","import torch\n","\n","import deepspeed\n","\n","video_modality = InputModalityWithEmbedding(\n","    name='video',\n","    input_channels=3,  # number of channels for each token of the input\n","    input_axis=3,  # number of axes, 3 for video)\n","    num_freq_bands=6,  # number of freq bands, with original value (2 * K + 1)\n","    max_freq=4.,  # maximum frequency, hyperparameter depending on how fine the data is\n",")\n","image_modality = InputModalityWithEmbedding(\n","    name='image',\n","    input_channels=3,  # number of channels for each token of the input\n","    input_axis=2,  # number of axes, 2 for images\n","    num_freq_bands=6,  # number of freq bands, with original value (2 * K + 1)\n","    max_freq=4.,  # maximum frequency, hyperparameter depending on how fine the data is\n",")\n","audio_modality = InputModalityWithEmbedding(\n","    name='audio',\n","    input_channels=1,  # number of channels for mono audio\n","    input_axis=1,  # number of axes, 2 for images\n","    num_freq_bands=6,  # number of freq bands, with original value (2 * K + 1)\n","    max_freq=8.,  # maximum frequency, hyperparameter depending on how fine the data is\n",")\n","model = MultiModalityWithTextPerceiver(\n","    modalities=(video_modality, image_modality),\n","    depth=2,  # depth of net, combined with num_latent_blocks_per_layer to produce full Perceiver\n","    num_latents=12,\n","    # number of latents, or induced set points, or centroids. different papers giving it different names\n","    latent_dim=64,  # latent dimension\n","    cross_heads=1,  # number of heads for cross attention. paper said 1\n","    latent_heads=2,  # number of heads for latent self attention, 8\n","    cross_dim_head=64,\n","    latent_dim_head=64,\n","    num_classes=10,  # output number of classes\n","    attn_dropout=0.,\n","    ff_dropout=0.,\n","    weight_tie_layers=True,\n","    num_latent_blocks_per_layer=2 # Note that this parameter is 1 in the original Lucidrain implementation\n","    # whether to weight tie layers (optional, as indicated in the diagram)\n",")\n","\n","\n","ds_config={    \"train_batch_size\": 3,\n","    \"steps_per_print\": 2000,\n","    \"optimizer\": {\n","      \"type\": \"Adam\",\n","      \"params\": {\n","        \"lr\": 0.001,\n","        \"betas\": [\n","          0.8,\n","          0.999\n","        ],\n","        \"eps\": 1e-8,\n","        \"weight_decay\": 3e-7\n","      }\n","    },\n","    \"fp16\": {\n","      \"enabled\": True,\n","      \"loss_scale\": 0,\n","      \"initial_scale_power\": 32,\n","      \"loss_scale_window\": 1000,\n","      \"hysteresis\": 2,\n","      \"min_loss_scale\": 1\n","    },\n","    \"scheduler\": {\n","      \"type\": \"WarmupLR\",\n","      \"params\": {\n","        \"warmup_min_lr\": 0,\n","        \"warmup_max_lr\": 0.001,\n","        \"warmup_num_steps\": 1000\n","      }\n","    },\n","    \"wall_clock_breakdown\": False\n","\n","  }\n","stage_3=True\n","if stage_3:\n","  ds_config.update({\n","      \"zero_optimization\": {\n","    \"stage\": 3,\n","    \"cpu_offload\": False,\n","    \"cpu_offload_params\": False,\n","    \"overlap_comm\": True,\n","    \"contiguous_gradients\": True,\n","    \"stage3_max_live_parameters\": 6000000,\n","    \"stage3_max_reuse_distance\": 100000000,\n","    \"stage3_prefetch_bucket_size\": 200000,\n","    \"stage3_param_persistence_threshold\": 100000,\n","    \"reduce_bucket_size\": 3000000,\n","    \"sub_group_size\": 1e6\n","  }})\n","model=model.to(torch.device('cuda'))\n","parameters = filter(lambda p: p.requires_grad, model.parameters())\n","# Initialize DeepSpeed to use the following features\n","# 1) Distributed model\n","# 2) Distributed data loader\n","# 3) DeepSpeed optimizer\n","model_engine, optimizer, trainloader, __ = deepspeed.initialize( model=model,\n","                                                                model_parameters=parameters,\n","                                                                config_params=ds_config\n",")\n","\n","for epoch in range(num_epochs):  # loop over the dataset multiple times\n","\n","      running_loss = 0.0\n","\n","      image_inputs= torch.rand(size=(3, 64, 64, 3), requires_grad=True).to(model_engine.local_rank)\n","      video_inputs= torch.rand(size=(3, 2, 64, 64, 3), requires_grad=True).to(model_engine.local_rank)\n","      with torch.cuda.amp.autocast():\n","        outputs = model_engine({\n","            'image': image_inputs,\n","            'video': video_inputs\n","            }\n","        )\n","      \n","      loss = outputs.mean()\n","\n","      model_engine.backward(loss)\n","      model_engine.step()\n","print(\"DONE\")\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[2021-04-18 14:14:19,654] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.3.14, git-hash=unknown, git-branch=unknown\n","[2021-04-18 14:14:19,662] [INFO] [engine.py:80:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n","Using /root/.cache/torch_extensions as PyTorch extensions root...\n","No modifications detected for re-loaded extension module fused_adam, skipping build step...\n","Loading extension module fused_adam...\n","Time to load fused_adam op: 0.0027871131896972656 seconds\n","[2021-04-18 14:14:19,777] [INFO] [engine.py:608:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n","[2021-04-18 14:14:19,778] [INFO] [engine.py:612:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n","Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n","[2021-04-18 14:14:19,782] [INFO] [logging.py:60:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n","Initializing ZeRO Stage 3\n","[2021-04-18 14:14:19,791] [WARNING] [stage3.py:35:<module>] apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.\n","[2021-04-18 14:14:19,852] [INFO] [utils.py:559:see_memory_usage] Stage 3 initialize beginning\n","[2021-04-18 14:14:19,860] [INFO] [utils.py:564:see_memory_usage] MA 0.01 GB         Max_MA 0.11 GB         CA 0.16 GB         Max_CA 0 GB \n","[2021-04-18 14:14:19,862] [INFO] [utils.py:569:see_memory_usage] CPU Virtual Memory:  used = 2.6 GB, percent = 20.4%\n","[2021-04-18 14:14:19,864] [INFO] [stage3.py:586:__init__] Reduce bucket size 3000000\n","[2021-04-18 14:14:19,872] [INFO] [stage3.py:587:__init__] Allgather bucket size 200000\n","Your linear layers are being patched with more memory efficient version. This will persit unless manually turned reset.\n","[2021-04-18 14:14:19,903] [INFO] [stage3.py:730:__init__] optimizer state initialized\n","[2021-04-18 14:14:19,982] [INFO] [utils.py:559:see_memory_usage] After initializing ZeRO optimizer\n","[2021-04-18 14:14:19,984] [INFO] [utils.py:564:see_memory_usage] MA 0.01 GB         Max_MA 0.01 GB         CA 0.16 GB         Max_CA 0 GB \n","[2021-04-18 14:14:19,986] [INFO] [utils.py:569:see_memory_usage] CPU Virtual Memory:  used = 2.6 GB, percent = 20.4%\n","[2021-04-18 14:14:19,988] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n","[2021-04-18 14:14:19,989] [INFO] [engine.py:445:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n","[2021-04-18 14:14:19,991] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f271a3de950>\n","[2021-04-18 14:14:19,992] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.8, 0.999]]\n","[2021-04-18 14:14:19,993] [INFO] [config.py:737:print] DeepSpeedEngine configuration:\n","[2021-04-18 14:14:19,994] [INFO] [config.py:741:print]   activation_checkpointing_config  {\n","    \"contiguous_memory_optimization\": false,\n","    \"cpu_checkpointing\": false,\n","    \"number_checkpoints\": null,\n","    \"partition_activations\": false,\n","    \"profile\": false,\n","    \"synchronize_checkpoint_boundary\": false\n","}\n","[2021-04-18 14:14:19,997] [INFO] [config.py:741:print]   allreduce_always_fp32 ........ False\n","[2021-04-18 14:14:19,999] [INFO] [config.py:741:print]   amp_enabled .................. False\n","[2021-04-18 14:14:20,002] [INFO] [config.py:741:print]   amp_params ................... False\n","[2021-04-18 14:14:20,005] [INFO] [config.py:741:print]   checkpoint_tag_validation_enabled  True\n","[2021-04-18 14:14:20,008] [INFO] [config.py:741:print]   checkpoint_tag_validation_fail  False\n","[2021-04-18 14:14:20,010] [INFO] [config.py:741:print]   disable_allgather ............ False\n","[2021-04-18 14:14:20,011] [INFO] [config.py:741:print]   dump_state ................... False\n","[2021-04-18 14:14:20,013] [INFO] [config.py:741:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n","[2021-04-18 14:14:20,015] [INFO] [config.py:741:print]   elasticity_enabled ........... False\n","[2021-04-18 14:14:20,016] [INFO] [config.py:741:print]   flops_profiler_config ........ {\n","    \"detailed\": true,\n","    \"enabled\": false,\n","    \"module_depth\": -1,\n","    \"profile_step\": 1,\n","    \"top_modules\": 3\n","}\n","[2021-04-18 14:14:20,018] [INFO] [config.py:741:print]   fp16_enabled ................. True\n","[2021-04-18 14:14:20,020] [INFO] [config.py:741:print]   global_rank .................. 0\n","[2021-04-18 14:14:20,021] [INFO] [config.py:741:print]   gradient_accumulation_steps .. 1\n","[2021-04-18 14:14:20,022] [INFO] [config.py:741:print]   gradient_clipping ............ 0.0\n","[2021-04-18 14:14:20,023] [INFO] [config.py:741:print]   gradient_predivide_factor .... 1.0\n","[2021-04-18 14:14:20,025] [INFO] [config.py:741:print]   initial_dynamic_scale ........ 4294967296\n","[2021-04-18 14:14:20,026] [INFO] [config.py:741:print]   loss_scale ................... 0\n","[2021-04-18 14:14:20,028] [INFO] [config.py:741:print]   memory_breakdown ............. False\n","[2021-04-18 14:14:20,030] [INFO] [config.py:741:print]   optimizer_legacy_fusion ...... False\n","[2021-04-18 14:14:20,031] [INFO] [config.py:741:print]   optimizer_name ............... adam\n","[2021-04-18 14:14:20,032] [INFO] [config.py:741:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n","[2021-04-18 14:14:20,034] [INFO] [config.py:741:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n","[2021-04-18 14:14:20,036] [INFO] [config.py:741:print]   pld_enabled .................. False\n","[2021-04-18 14:14:20,037] [INFO] [config.py:741:print]   pld_params ................... False\n","[2021-04-18 14:14:20,038] [INFO] [config.py:741:print]   prescale_gradients ........... False\n","[2021-04-18 14:14:20,040] [INFO] [config.py:741:print]   scheduler_name ............... WarmupLR\n","[2021-04-18 14:14:20,041] [INFO] [config.py:741:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\n","[2021-04-18 14:14:20,043] [INFO] [config.py:741:print]   sparse_attention ............. None\n","[2021-04-18 14:14:20,044] [INFO] [config.py:741:print]   sparse_gradients_enabled ..... False\n","[2021-04-18 14:14:20,046] [INFO] [config.py:741:print]   steps_per_print .............. 2000\n","[2021-04-18 14:14:20,047] [INFO] [config.py:741:print]   tensorboard_enabled .......... False\n","[2021-04-18 14:14:20,049] [INFO] [config.py:741:print]   tensorboard_job_name ......... DeepSpeedJobName\n","[2021-04-18 14:14:20,050] [INFO] [config.py:741:print]   tensorboard_output_path ...... \n","[2021-04-18 14:14:20,052] [INFO] [config.py:741:print]   train_batch_size ............. 3\n","[2021-04-18 14:14:20,054] [INFO] [config.py:741:print]   train_micro_batch_size_per_gpu  3\n","[2021-04-18 14:14:20,055] [INFO] [config.py:741:print]   wall_clock_breakdown ......... False\n","[2021-04-18 14:14:20,057] [INFO] [config.py:741:print]   world_size ................... 1\n","[2021-04-18 14:14:20,058] [INFO] [config.py:741:print]   zero_allow_untested_optimizer  False\n","[2021-04-18 14:14:20,060] [INFO] [config.py:741:print]   zero_config .................. {\n","    \"allgather_bucket_size\": 500000000,\n","    \"allgather_partitions\": true,\n","    \"contiguous_gradients\": true,\n","    \"cpu_offload\": false,\n","    \"cpu_offload_params\": false,\n","    \"cpu_offload_use_pin_memory\": false,\n","    \"elastic_checkpoint\": true,\n","    \"gather_fp16_weights_on_model_save\": false,\n","    \"load_from_fp32_weights\": true,\n","    \"max_live_parameters\": 6000000,\n","    \"max_reuse_distance\": 100000000,\n","    \"overlap_comm\": true,\n","    \"param_persistence_threshold\": 100000,\n","    \"prefetch_bucket_size\": 200000,\n","    \"reduce_bucket_size\": 3000000,\n","    \"reduce_scatter\": true,\n","    \"stage\": 3,\n","    \"sub_group_size\": 1000000.0\n","}\n","[2021-04-18 14:14:20,061] [INFO] [config.py:741:print]   zero_enabled ................. True\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:346: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n","  FutureWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/cuda/memory.py:354: FutureWarning: torch.cuda.max_memory_cached has been renamed to torch.cuda.max_memory_reserved\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[2021-04-18 14:14:20,063] [INFO] [config.py:741:print]   zero_optimization_stage ...... 3\n","[2021-04-18 14:14:20,064] [INFO] [config.py:747:print]   json = {\n","    \"fp16\":{\n","        \"enabled\":true,\n","        \"hysteresis\":2,\n","        \"initial_scale_power\":32,\n","        \"loss_scale\":0,\n","        \"loss_scale_window\":1000,\n","        \"min_loss_scale\":1\n","    },\n","    \"optimizer\":{\n","        \"params\":{\n","            \"betas\":[\n","                0.8,\n","                0.999\n","            ],\n","            \"eps\":1e-08,\n","            \"lr\":0.001,\n","            \"weight_decay\":3e-07\n","        },\n","        \"type\":\"Adam\"\n","    },\n","    \"scheduler\":{\n","        \"params\":{\n","            \"warmup_max_lr\":0.001,\n","            \"warmup_min_lr\":0,\n","            \"warmup_num_steps\":1000\n","        },\n","        \"type\":\"WarmupLR\"\n","    },\n","    \"steps_per_print\":2000,\n","    \"train_batch_size\":3,\n","    \"wall_clock_breakdown\":false,\n","    \"zero_optimization\":{\n","        \"contiguous_gradients\":true,\n","        \"cpu_offload\":false,\n","        \"cpu_offload_params\":false,\n","        \"overlap_comm\":true,\n","        \"reduce_bucket_size\":3000000,\n","        \"stage\":3,\n","        \"stage3_max_live_parameters\":6000000,\n","        \"stage3_max_reuse_distance\":100000000,\n","        \"stage3_param_persistence_threshold\":100000,\n","        \"stage3_prefetch_bucket_size\":200000,\n","        \"sub_group_size\":1000000.0\n","    }\n","}\n","Using /root/.cache/torch_extensions as PyTorch extensions root...\n","No modifications detected for re-loaded extension module utils, skipping build step...\n","Loading extension module utils...\n","Time to load utils op: 0.0007736682891845703 seconds\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c1cb01acf5d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m       \u001b[0mmodel_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m       \u001b[0mmodel_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DONE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/engine.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, allreduce_gradients, release_loss)\u001b[0m\n\u001b[1;32m    995\u001b[0m             self.optimizer.is_gradient_accumulation_boundary = self.is_gradient_accumulation_boundary(\n\u001b[1;32m    996\u001b[0m             )\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;31m# AMP requires delaying unscale when inside gradient accumulation boundaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/stage3.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m   2553\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipg_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m         '''Partitioning Parameters that were not partitioned\n\u001b[1;32m   2557\u001b[0m         \u001b[0mUsually\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0mwhose\u001b[0m \u001b[0minput\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/fp16/loss_scaler.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mscaled_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# _forward_cls is defined by derived class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/deepspeed/runtime/zero/linear.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     83\u001b[0m                                                                     input.shape[-1]))\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mgrad_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;31m#print(f\"Computed grad weight grad_weight {grad_weight.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Half but found Float"]}]},{"cell_type":"code","metadata":{"id":"3EZhS-DZT18B"},"source":["ONE"],"execution_count":null,"outputs":[]}]}